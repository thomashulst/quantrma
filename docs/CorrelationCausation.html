<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title> 3 Week 3: Correlation and Causation | Quantitative Research Methods &amp; Analysis</title>
  <meta name="description" content="An introductory textbook for quantitative research methods and data analysis at EUC. This text was adapted from “Answering questions with data” by Matthew J.C. Crump (https://crumplab.github.io/statistics/)" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content=" 3 Week 3: Correlation and Causation | Quantitative Research Methods &amp; Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="An introductory textbook for quantitative research methods and data analysis at EUC. This text was adapted from “Answering questions with data” by Matthew J.C. Crump (https://crumplab.github.io/statistics/)" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content=" 3 Week 3: Correlation and Causation | Quantitative Research Methods &amp; Analysis" />
  
  <meta name="twitter:description" content="An introductory textbook for quantitative research methods and data analysis at EUC. This text was adapted from “Answering questions with data” by Matthew J.C. Crump (https://crumplab.github.io/statistics/)" />
  

<meta name="author" content="Lead Author: Matthew J. C. Crump" />
<meta name="author" content="Chapters 2 and 4 adapted from Navarro, D." />
<meta name="author" content="Videos: Jeffrey Suzuki" />
<meta name="author" content="Adapted for use at EUC by Thomas Hulst" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="DescribingData.html"/>
<link rel="next" href="for-thanos-to-look-at.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="tufte.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">QuantRMA</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#important-notes"><i class="fa fa-check"></i><b>0.1</b> Important notes</a><ul>
<li class="chapter" data-level="0.1.1" data-path="index.html"><a href="index.html#attributions"><i class="fa fa-check"></i><b>0.1.1</b> Attributions</a></li>
<li class="chapter" data-level="0.1.2" data-path="index.html"><a href="index.html#cc-by-sa-4.0-license"><i class="fa fa-check"></i><b>0.1.2</b> CC BY-SA 4.0 license</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="Introduction.html"><a href="Introduction.html"><i class="fa fa-check"></i><b>1</b> Week 1: Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="Introduction.html"><a href="Introduction.html#the-curse-of-belief-bias"><i class="fa fa-check"></i><b>1.1</b> The curse of belief bias</a></li>
<li class="chapter" data-level="1.2" data-path="Introduction.html"><a href="Introduction.html#the-cautionary-tale-of-simpsons-paradox"><i class="fa fa-check"></i><b>1.2</b> The cautionary tale of Simpson’s paradox</a></li>
<li class="chapter" data-level="1.3" data-path="Introduction.html"><a href="Introduction.html#statistics-in-psychology"><i class="fa fa-check"></i><b>1.3</b> Statistics in psychology</a></li>
<li class="chapter" data-level="1.4" data-path="Introduction.html"><a href="Introduction.html#statistics-in-everyday-life"><i class="fa fa-check"></i><b>1.4</b> Statistics in everyday life</a></li>
<li class="chapter" data-level="1.5" data-path="Introduction.html"><a href="Introduction.html#theres-more-to-research-methods-than-statistics"><i class="fa fa-check"></i><b>1.5</b> There’s more to research methods than statistics</a></li>
<li class="chapter" data-level="1.6" data-path="Introduction.html"><a href="Introduction.html#a-brief-introduction-to-research-methods"><i class="fa fa-check"></i><b>1.6</b> A brief introduction to research methods</a></li>
<li class="chapter" data-level="1.7" data-path="Introduction.html"><a href="Introduction.html#some-thoughts-about-measurement"><i class="fa fa-check"></i><b>1.7</b> Some thoughts about measurement</a></li>
<li class="chapter" data-level="1.8" data-path="Introduction.html"><a href="Introduction.html#operationalization-defining-your-measurement"><i class="fa fa-check"></i><b>1.8</b> Operationalization: defining your measurement</a></li>
<li class="chapter" data-level="1.9" data-path="Introduction.html"><a href="Introduction.html#assessing-the-reliability-of-a-measurement"><i class="fa fa-check"></i><b>1.9</b> Assessing the reliability of a measurement</a></li>
<li class="chapter" data-level="1.10" data-path="Introduction.html"><a href="Introduction.html#the-role-of-variables-predictors-and-outcomes"><i class="fa fa-check"></i><b>1.10</b> The role of variables: predictors and outcomes</a></li>
<li class="chapter" data-level="1.11" data-path="Introduction.html"><a href="Introduction.html#rm-content-still-missing-in-week-1-according-to-thanos"><i class="fa fa-check"></i><b>1.11</b> RM content still missing in week 1 according to Thanos</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="DescribingData.html"><a href="DescribingData.html"><i class="fa fa-check"></i><b>2</b> Week 2: Describing Data</a><ul>
<li class="chapter" data-level="2.1" data-path="DescribingData.html"><a href="DescribingData.html#this-is-what-too-many-numbers-looks-like"><i class="fa fa-check"></i><b>2.1</b> This is what too many numbers looks like</a></li>
<li class="chapter" data-level="2.2" data-path="DescribingData.html"><a href="DescribingData.html#look-at-the-data"><i class="fa fa-check"></i><b>2.2</b> Look at the data</a><ul>
<li class="chapter" data-level="2.2.1" data-path="DescribingData.html"><a href="DescribingData.html#stop-plotting-time-o-o-oh-u-can-plot-this"><i class="fa fa-check"></i><b>2.2.1</b> Stop, plotting time (o o oh) U can plot this</a></li>
<li class="chapter" data-level="2.2.2" data-path="DescribingData.html"><a href="DescribingData.html#histograms"><i class="fa fa-check"></i><b>2.2.2</b> Histograms</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="DescribingData.html"><a href="DescribingData.html#important-ideas-distribution-central-tendency-and-variance"><i class="fa fa-check"></i><b>2.3</b> Important Ideas: Distribution, Central Tendency, and Variance</a></li>
<li class="chapter" data-level="2.4" data-path="DescribingData.html"><a href="DescribingData.html#measures-of-central-tendency-sameness"><i class="fa fa-check"></i><b>2.4</b> Measures of Central Tendency (Sameness)</a><ul>
<li class="chapter" data-level="2.4.1" data-path="DescribingData.html"><a href="DescribingData.html#from-many-numbers-to-one"><i class="fa fa-check"></i><b>2.4.1</b> From many numbers to one</a></li>
<li class="chapter" data-level="2.4.2" data-path="DescribingData.html"><a href="DescribingData.html#mode"><i class="fa fa-check"></i><b>2.4.2</b> Mode</a></li>
<li class="chapter" data-level="2.4.3" data-path="DescribingData.html"><a href="DescribingData.html#median"><i class="fa fa-check"></i><b>2.4.3</b> Median</a></li>
<li class="chapter" data-level="2.4.4" data-path="DescribingData.html"><a href="DescribingData.html#mean"><i class="fa fa-check"></i><b>2.4.4</b> Mean</a></li>
<li class="chapter" data-level="2.4.5" data-path="DescribingData.html"><a href="DescribingData.html#what-does-the-mean-mean"><i class="fa fa-check"></i><b>2.4.5</b> What does the mean mean?</a></li>
<li class="chapter" data-level="2.4.6" data-path="DescribingData.html"><a href="DescribingData.html#all-together-now"><i class="fa fa-check"></i><b>2.4.6</b> All together now</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="DescribingData.html"><a href="DescribingData.html#measures-of-variation-differentness"><i class="fa fa-check"></i><b>2.5</b> Measures of Variation (Differentness)</a><ul>
<li class="chapter" data-level="2.5.1" data-path="DescribingData.html"><a href="DescribingData.html#the-range"><i class="fa fa-check"></i><b>2.5.1</b> The Range</a></li>
<li class="chapter" data-level="2.5.2" data-path="DescribingData.html"><a href="DescribingData.html#the-difference-scores"><i class="fa fa-check"></i><b>2.5.2</b> The Difference Scores</a></li>
<li class="chapter" data-level="2.5.3" data-path="DescribingData.html"><a href="DescribingData.html#the-variance"><i class="fa fa-check"></i><b>2.5.3</b> The Variance</a></li>
<li class="chapter" data-level="2.5.4" data-path="DescribingData.html"><a href="DescribingData.html#the-standard-deviation"><i class="fa fa-check"></i><b>2.5.4</b> The Standard Deviation</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="DescribingData.html"><a href="DescribingData.html#using-descriptive-statistics-with-data"><i class="fa fa-check"></i><b>2.6</b> Using Descriptive Statistics with data</a></li>
<li class="chapter" data-level="2.7" data-path="DescribingData.html"><a href="DescribingData.html#rolling-your-own-descriptive-statistics"><i class="fa fa-check"></i><b>2.7</b> Rolling your own descriptive statistics</a><ul>
<li class="chapter" data-level="2.7.1" data-path="DescribingData.html"><a href="DescribingData.html#absolute-deviations"><i class="fa fa-check"></i><b>2.7.1</b> Absolute deviations</a></li>
<li class="chapter" data-level="2.7.2" data-path="DescribingData.html"><a href="DescribingData.html#other-sign-inverting-operations"><i class="fa fa-check"></i><b>2.7.2</b> Other sign-inverting operations</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="DescribingData.html"><a href="DescribingData.html#remember-to-look-at-your-data"><i class="fa fa-check"></i><b>2.8</b> Remember to look at your data</a><ul>
<li class="chapter" data-level="2.8.1" data-path="DescribingData.html"><a href="DescribingData.html#anscombes-quartet"><i class="fa fa-check"></i><b>2.8.1</b> Anscombe’s Quartet</a></li>
<li class="chapter" data-level="2.8.2" data-path="DescribingData.html"><a href="DescribingData.html#datasaurus-dozen"><i class="fa fa-check"></i><b>2.8.2</b> Datasaurus Dozen</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="DescribingData.html"><a href="DescribingData.html#videos"><i class="fa fa-check"></i><b>2.9</b> Videos</a><ul>
<li class="chapter" data-level="2.9.1" data-path="DescribingData.html"><a href="DescribingData.html#measures-of-center-mode"><i class="fa fa-check"></i><b>2.9.1</b> Measures of center: Mode</a></li>
<li class="chapter" data-level="2.9.2" data-path="DescribingData.html"><a href="DescribingData.html#measures-of-center-median-and-mean"><i class="fa fa-check"></i><b>2.9.2</b> Measures of center: Median and Mean</a></li>
<li class="chapter" data-level="2.9.3" data-path="DescribingData.html"><a href="DescribingData.html#standard-deviation-part-i"><i class="fa fa-check"></i><b>2.9.3</b> Standard deviation part I</a></li>
<li class="chapter" data-level="2.9.4" data-path="DescribingData.html"><a href="DescribingData.html#standard-deviation-part-ii"><i class="fa fa-check"></i><b>2.9.4</b> Standard deviation part II</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="CorrelationCausation.html"><a href="CorrelationCausation.html"><i class="fa fa-check"></i><b>3</b> Week 3: Correlation and Causation</a><ul>
<li class="chapter" data-level="3.1" data-path="CorrelationCausation.html"><a href="CorrelationCausation.html#if-something-caused-something-else-to-change-what-would-that-look-like"><i class="fa fa-check"></i><b>3.1</b> If something caused something else to change, what would that look like?</a><ul>
<li class="chapter" data-level="3.1.1" data-path="CorrelationCausation.html"><a href="CorrelationCausation.html#charlie-and-the-chocolate-factory"><i class="fa fa-check"></i><b>3.1.1</b> Charlie and the Chocolate factory</a></li>
<li class="chapter" data-level="3.1.2" data-path="CorrelationCausation.html"><a href="CorrelationCausation.html#scatter-plots"><i class="fa fa-check"></i><b>3.1.2</b> Scatter plots</a></li>
<li class="chapter" data-level="3.1.3" data-path="CorrelationCausation.html"><a href="CorrelationCausation.html#positive-negative-and-no-correlation"><i class="fa fa-check"></i><b>3.1.3</b> Positive, Negative, and No-Correlation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="CorrelationCausation.html"><a href="CorrelationCausation.html#pearsons-r"><i class="fa fa-check"></i><b>3.2</b> Pearson’s r</a><ul>
<li class="chapter" data-level="3.2.1" data-path="CorrelationCausation.html"><a href="CorrelationCausation.html#the-idea-of-co-variance"><i class="fa fa-check"></i><b>3.2.1</b> The idea of co-variance</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="CorrelationCausation.html"><a href="CorrelationCausation.html#turning-the-numbers-into-a-measure-of-co-variance"><i class="fa fa-check"></i><b>3.3</b> Turning the numbers into a measure of co-variance</a><ul>
<li class="chapter" data-level="3.3.1" data-path="CorrelationCausation.html"><a href="CorrelationCausation.html#co-variance-the-measure"><i class="fa fa-check"></i><b>3.3.1</b> Co-variance, the measure</a></li>
<li class="chapter" data-level="3.3.2" data-path="CorrelationCausation.html"><a href="CorrelationCausation.html#r-we-there-yet"><i class="fa fa-check"></i><b>3.3.2</b> r we there yet?</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="CorrelationCausation.html"><a href="CorrelationCausation.html#examples-with-data"><i class="fa fa-check"></i><b>3.4</b> Examples with Data</a></li>
<li class="chapter" data-level="3.5" data-path="CorrelationCausation.html"><a href="CorrelationCausation.html#interpreting-correlations"><i class="fa fa-check"></i><b>3.5</b> Interpreting Correlations</a><ul>
<li class="chapter" data-level="3.5.1" data-path="CorrelationCausation.html"><a href="CorrelationCausation.html#correlation-does-not-equal-causation"><i class="fa fa-check"></i><b>3.5.1</b> Correlation does not equal causation</a></li>
<li class="chapter" data-level="3.5.2" data-path="CorrelationCausation.html"><a href="CorrelationCausation.html#correlation-and-random-chance"><i class="fa fa-check"></i><b>3.5.2</b> Correlation and random chance</a></li>
<li class="chapter" data-level="3.5.3" data-path="CorrelationCausation.html"><a href="CorrelationCausation.html#some-more-animations"><i class="fa fa-check"></i><b>3.5.3</b> Some more animations</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="CorrelationCausation.html"><a href="CorrelationCausation.html#summary"><i class="fa fa-check"></i><b>3.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="for-thanos-to-look-at.html"><a href="for-thanos-to-look-at.html"><i class="fa fa-check"></i><b>4</b> FOR THANOS TO LOOK AT</a><ul>
<li class="chapter" data-level="4.1" data-path="for-thanos-to-look-at.html"><a href="for-thanos-to-look-at.html#scales-of-measurement-week-2"><i class="fa fa-check"></i><b>4.1</b> Scales of measurement (week 2?)</a><ul>
<li class="chapter" data-level="4.1.1" data-path="for-thanos-to-look-at.html"><a href="for-thanos-to-look-at.html#nominal-scale"><i class="fa fa-check"></i><b>4.1.1</b> Nominal scale</a></li>
<li class="chapter" data-level="4.1.2" data-path="for-thanos-to-look-at.html"><a href="for-thanos-to-look-at.html#ordinal-scale"><i class="fa fa-check"></i><b>4.1.2</b> Ordinal scale</a></li>
<li class="chapter" data-level="4.1.3" data-path="for-thanos-to-look-at.html"><a href="for-thanos-to-look-at.html#interval-scale"><i class="fa fa-check"></i><b>4.1.3</b> Interval scale</a></li>
<li class="chapter" data-level="4.1.4" data-path="for-thanos-to-look-at.html"><a href="for-thanos-to-look-at.html#ratio-scale"><i class="fa fa-check"></i><b>4.1.4</b> Ratio scale</a></li>
<li class="chapter" data-level="4.1.5" data-path="for-thanos-to-look-at.html"><a href="for-thanos-to-look-at.html#continuous-versus-discrete-variables"><i class="fa fa-check"></i><b>4.1.5</b> Continuous versus discrete variables</a></li>
<li class="chapter" data-level="4.1.6" data-path="for-thanos-to-look-at.html"><a href="for-thanos-to-look-at.html#some-complexities"><i class="fa fa-check"></i><b>4.1.6</b> Some complexities</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="for-thanos-to-look-at.html"><a href="for-thanos-to-look-at.html#experimental-and-non-experimental-research-week-3"><i class="fa fa-check"></i><b>4.2</b> Experimental and non-experimental research (week 3?)</a><ul>
<li class="chapter" data-level="4.2.1" data-path="for-thanos-to-look-at.html"><a href="for-thanos-to-look-at.html#experimental-research"><i class="fa fa-check"></i><b>4.2.1</b> Experimental research</a></li>
<li class="chapter" data-level="4.2.2" data-path="for-thanos-to-look-at.html"><a href="for-thanos-to-look-at.html#non-experimental-research"><i class="fa fa-check"></i><b>4.2.2</b> Non-experimental research</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="for-thanos-to-look-at.html"><a href="for-thanos-to-look-at.html#assessing-the-validity-of-a-study-week-1-or-2-or-3"><i class="fa fa-check"></i><b>4.3</b> Assessing the validity of a study (week 1?? or 2?? or 3??)</a><ul>
<li class="chapter" data-level="4.3.1" data-path="for-thanos-to-look-at.html"><a href="for-thanos-to-look-at.html#internal-validity"><i class="fa fa-check"></i><b>4.3.1</b> Internal validity</a></li>
<li class="chapter" data-level="4.3.2" data-path="for-thanos-to-look-at.html"><a href="for-thanos-to-look-at.html#external-validity"><i class="fa fa-check"></i><b>4.3.2</b> External validity</a></li>
<li class="chapter" data-level="4.3.3" data-path="for-thanos-to-look-at.html"><a href="for-thanos-to-look-at.html#construct-validity"><i class="fa fa-check"></i><b>4.3.3</b> Construct validity</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="for-thanos-to-look-at.html"><a href="for-thanos-to-look-at.html#confounds-artifacts-and-other-threats-to-validity-week-3"><i class="fa fa-check"></i><b>4.4</b> Confounds, artifacts and other threats to validity (week 3?)</a><ul>
<li class="chapter" data-level="4.4.1" data-path="for-thanos-to-look-at.html"><a href="for-thanos-to-look-at.html#history-effects"><i class="fa fa-check"></i><b>4.4.1</b> History effects</a></li>
<li class="chapter" data-level="4.4.2" data-path="for-thanos-to-look-at.html"><a href="for-thanos-to-look-at.html#maturation-effects"><i class="fa fa-check"></i><b>4.4.2</b> Maturation effects</a></li>
<li class="chapter" data-level="4.4.3" data-path="for-thanos-to-look-at.html"><a href="for-thanos-to-look-at.html#repeated-testing-effects"><i class="fa fa-check"></i><b>4.4.3</b> Repeated testing effects</a></li>
<li class="chapter" data-level="4.4.4" data-path="for-thanos-to-look-at.html"><a href="for-thanos-to-look-at.html#selection-bias"><i class="fa fa-check"></i><b>4.4.4</b> Selection bias</a></li>
<li class="chapter" data-level="4.4.5" data-path="for-thanos-to-look-at.html"><a href="for-thanos-to-look-at.html#differential-attrition"><i class="fa fa-check"></i><b>4.4.5</b> Differential attrition</a></li>
<li class="chapter" data-level="4.4.6" data-path="for-thanos-to-look-at.html"><a href="for-thanos-to-look-at.html#non-response-bias"><i class="fa fa-check"></i><b>4.4.6</b> Non-response bias</a></li>
<li class="chapter" data-level="4.4.7" data-path="for-thanos-to-look-at.html"><a href="for-thanos-to-look-at.html#regression-to-the-mean"><i class="fa fa-check"></i><b>4.4.7</b> Regression to the mean</a></li>
<li class="chapter" data-level="4.4.8" data-path="for-thanos-to-look-at.html"><a href="for-thanos-to-look-at.html#experimenter-bias"><i class="fa fa-check"></i><b>4.4.8</b> Experimenter bias</a></li>
<li class="chapter" data-level="4.4.9" data-path="for-thanos-to-look-at.html"><a href="for-thanos-to-look-at.html#demand-effects-and-reactivity"><i class="fa fa-check"></i><b>4.4.9</b> Demand effects and reactivity</a></li>
<li class="chapter" data-level="4.4.10" data-path="for-thanos-to-look-at.html"><a href="for-thanos-to-look-at.html#placebo-effects"><i class="fa fa-check"></i><b>4.4.10</b> Placebo effects</a></li>
<li class="chapter" data-level="4.4.11" data-path="for-thanos-to-look-at.html"><a href="for-thanos-to-look-at.html#situation-measurement-and-subpopulation-effects"><i class="fa fa-check"></i><b>4.4.11</b> Situation, measurement and subpopulation effects</a></li>
<li class="chapter" data-level="4.4.12" data-path="for-thanos-to-look-at.html"><a href="for-thanos-to-look-at.html#fraud-deception-and-self-deception"><i class="fa fa-check"></i><b>4.4.12</b> Fraud, deception and self-deception</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Quantitative Research Methods &amp; Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="CorrelationCausation" class="section level1">
<h1><span class="header-section-number"> 3</span> Week 3: Correlation and Causation</h1>
<p><span class="newthought">
Correlation does not equal causation
—Every Statistics and Research Methods Instructor Ever
</span></p>
<div class="marginnote">
<p>Chapter by Matthew Crump</p>
</div>
<p>In the last chapter we had some data. It was too much too look at and it didn’t make sense. So, we talked about how to look at the data visually using plots and histograms, and we talked about how to summarize lots of numbers so we could determine their central tendencies (sameness) and variability (differentness). And, all was well with the world.</p>
<p>Let’s not forget the big reason why we learned about descriptive statistics. The big reason is that we are interested in getting answers to questions using data. If you are looking for a big theme to think about while you take this course, the theme is: how do we ask and answer questions using data?</p>
<p>For every section in this book, you should be connecting your inner monologue to this question, and asking yourself: How does what I am learning about help me answer questions with data? Advance warning: we know it is easy to forget this stuff when we dive into the details, and we will try to throw you a rope to help you out along the way…remember, we’re trying to answer questions with data.</p>
<p>We started Chapter two with some fake data on human happiness, remember? We imagined that we asked a bunch of people to tell us how happy they were, then we looked at the numbers they gave us. Let’s continue with this imaginary thought experiment.</p>
<p>What do you get when you ask people to use a number to describe how happy they are? A bunch of numbers. What kind of questions can you ask about those numbers? Well, you can look at the numbers and estimate their general properties as we already did. We would expect those numbers tell us some things we already know. There are different people, and different people are different amounts of happy. You’ve probably met some of those of really happy people, and really unhappy people, and you yourself probably have some amount of happiness. “Great, thanks Captain Obvious”.</p>
<p>Before moving on, you should also be skeptical of what the numbers might mean. For example, if you force people to give a number between 0-100 to rate their happiness, does this number truly reflect how happy that person is? Can a person know how happy they are? Does the question format bias how they give their answer? Is happiness even a real thing? These are all good questions about the <strong>validity</strong> of the construct (happiness itself) and the measure (numbers) you are using to quantify it. For now, though, we will side-step those very important questions, and assume that, happiness is a thing, and our measure of happiness measures something about how happy people are.</p>
<p>OK then, after we have measured some happiness, I bet you can think of some more pressing questions. For example, what causes happiness to go up or down? If you knew the causes of happiness what could you do? How about increase your own happiness; or, help people who are unhappy; or, better appreciate why Eeyore from Winnie the Pooh is unhappy; or, present valid scientific arguments that argue against incorrect claims about what causes happiness. A causal theory and understanding of happiness could be used for all of those things. How can we get there?</p>
<p>Imagine you were an alien observer. You arrived on earth and heard about this thing called happiness that people have. You want to know what causes happiness. You also discover that planet earth has lots of other things. Which of those things, you wonder, cause happiness? How would your alien-self get started on this big question.</p>
<p>As a person who has happiness, you might already have some hunches about what causes changes in happiness. For example things like: weather, friends, music, money, education, drugs, books, movies, belieifs, personality, color of your shoes, eyebrow length, number of cat’s you see per day, frequency of subway delay, a lifetime supply of chocolate, etcetera etcetera (as Willy Wonka would say), might all contribute to happiness in someway. There could be many different causes of happiness.</p>
<div id="if-something-caused-something-else-to-change-what-would-that-look-like" class="section level2">
<h2><span class="header-section-number">3.1</span> If something caused something else to change, what would that look like?</h2>
<p>Before we go around determining the causes of happiness, we should prepare ourselves with some analytical tools so that we could identify what causation looks like. If we don’t prepare ourselves for what we might find, then we won’t know how to interpret our own data. Instead, we need to anticipate what the data could look like. Specifically, we need to know what data would look like when one thing does not cause another thing, and what data would look like when one thing does cause another thing. This chapter does some of this preparation. Fair warning: we will find out some tricky things. For example, we can find patterns that look like one thing is causing another, even when that one thing DOES NOT CAUSE the other thing. Hang in there.</p>
<div id="charlie-and-the-chocolate-factory" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Charlie and the Chocolate factory</h3>
<p>Let’s imagine that a person’s supply of chocolate has a causal influence on their level of happiness. Let’s further imagine that, like Charlie, the more chocolate you have the more happy you will be, and the less chocolate you have, the less happy you will be. Finally, because we suspect happiness is caused by lots of other things in a person’s life, we anticipate that the relationship between chocolate supply and happiness won’t be perfect. What do these assumptions mean for how the data should look?</p>
<p>Our first step is to collect some imaginary data from 100 people. We walk around and ask the first 100 people we meet to answer two questions:</p>
<ol style="list-style-type: decimal">
<li>how much chocolate do you have, and</li>
<li>how happy are you.</li>
</ol>
<p>For convenience, both the scales will go from 0 to 100. For the chocolate scale, 0 means no chocolate, 100 means lifetime supply of chocolate. Any other number is somewhere in between. For the happiness scale, 0 means no happiness, 100 means all of the happiness, and in between means some amount in between.</p>
<p>Here is some sample data from the first 10 imaginary subjects.</p>
<table>
<thead>
<tr class="header">
<th align="right">subject</th>
<th align="right">chocolate</th>
<th align="right">happiness</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">1</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">2</td>
<td align="right">2</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">2</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">3</td>
<td align="right">2</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">3</td>
<td align="right">5</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">4</td>
<td align="right">5</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="right">5</td>
<td align="right">5</td>
</tr>
<tr class="even">
<td align="right">8</td>
<td align="right">7</td>
<td align="right">6</td>
</tr>
<tr class="odd">
<td align="right">9</td>
<td align="right">5</td>
<td align="right">5</td>
</tr>
<tr class="even">
<td align="right">10</td>
<td align="right">9</td>
<td align="right">8</td>
</tr>
</tbody>
</table>
<p>We asked each subject two questions so there are two scores for each subject, one for their chocolate supply, and one for their level of happiness. You might already notice some relationships between amount of chocolate and level of happiness in the table. To make those relationships even more clear, let’s plot all of the data in a graph.</p>
</div>
<div id="scatter-plots" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Scatter plots</h3>
<p>When you have two measurements worth of data, you can always turn them into dots and plot them in a scatter plot. A scatter plot has a horizontal x-axis, and a vertical y-axis. You get to choose which measurement goes on which axis. Let’s put chocolate supply on the x-axis, and happiness level on the y-axis. The plot below shows 100 dots for each subject.</p>
<div class="figure"><span id="fig:3scatter1"></span>
<img src="quantrma_files/figure-html/3scatter1-1.png" alt="Imaginary data showing a positive correlation between amount of chocolate and amount happiness" width="672" />
<p class="caption">
Figure 3.1: Imaginary data showing a positive correlation between amount of chocolate and amount happiness
</p>
</div>
<p>You might be wondering, why are there only 100 dots for the data. Didn’t we collect 100 measures for chocolate, and 100 measures for happiness, shouldn’t there be 200 dots? Nope. Each dot is for one subject, there are 100 subjects, so there are 100 dots.</p>
<p>What do the dots mean? Each dot has two coordinates, an x-coordinate for chocolate, and a y-coordinate for happiness. The first dot, all the way on the bottom left is the first subject in the table, who had close to 0 chocolate and close to zero happiness. You can look at any dot, then draw a straight line down to the x-axis: that will tell you how much chocolate that subject has. You can draw a straight line left to the y-axis: that will tell you how much happiness the subject has.</p>
<p>Now that we are looking at the scatter plot, we can see many things. The dots are scattered around a bit, hence the name <strong>scatter plot</strong>. Even when the dots don’t scatter, they’re still called scatter plots, perhaps because those pesky dots in real life have so much scatter all the time. More important, the dots show a relationship between chocolate supply and happiness. Happiness is lower for people with smaller supplies of chocolate, and higher for people with larger supplies of chocolate. It looks like the more chocolate you have the happier you will be, and vice-versa. This kind of relationship is called a <strong>positive correlation</strong>.</p>
</div>
<div id="positive-negative-and-no-correlation" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Positive, Negative, and No-Correlation</h3>
<p>Seeing as we are in the business of imagining data, let’s imagine some more. We’ve already imagined what data would look like if larger chocolate supplies increase happiness. We’ll show that again in a bit. What do you imagine the scatter plot would look like if the relationship was reversed, and larger chocolate supplies decreased happiness. Or, what do you imagine the scatter plot would look like if there was no relationship, and the amount of chocolate that you have doesn’t do anything to your happiness. We invite your imagination to look at these graphs:</p>
<div class="figure"><span id="fig:3posnegrand"></span>
<img src="quantrma_files/figure-html/3posnegrand-1.png" alt="Three scatterplots showing negative, positive, and zero correlation" width="672" />
<p class="caption">
Figure 3.2: Three scatterplots showing negative, positive, and zero correlation
</p>
</div>
<p>The first panel shows a <strong>negative correlation</strong>. Happiness goes down as chocolate supply increases. Negative correlation occurs when one thing goes up and the other thing goes down; or, when more of X is less of Y, and vice-versa. The second panel shows a <strong>positive correlation</strong>. Happiness goes up as chocolate as chocolate supply increases. Positive correlation occurs when both things go up together, and go down together: more of X is more of Y, and vice-versa. The third panel shows <strong>no correlation</strong>. Here, there doesn’t appear to be any obvious relationship between chocolate supply and happiness. The dots are scattered all over the place, the truest of the scatter plots.</p>
<div class="marginnote">
<p>We are wading into the idea that measures of two things can be related, or correlated with one another. It is possible for the relationships to be more complicated than just going up, or going down. For example, we could have a relationship that where the dots go up for the first half of X, and then go down for the second half.</p>
</div>
<p>Zero correlation occurs when one thing is not related in any way to another things: changes in X do not relate to any changes in Y, and vice-versa.</p>
</div>
</div>
<div id="pearsons-r" class="section level2">
<h2><span class="header-section-number">3.2</span> Pearson’s r</h2>
<div class="marginnote">
<p>The stories about the invention of various statistics are very interesting, you can read more about them in the book, “The Lady Tasting Tea” <span class="citation">(Salsburg <a href="#ref-salsburg2001lady" role="doc-biblioref">2001</a>)</span></p>
</div>
<p>If Beyoncé was a statistician, she might look at these scatter plots and want to “put a number on it”. We think this is a good idea too. We’ve already learned how to create descriptive statistics for a single measure, like chocolate, or happiness (i.e., means, variances, etc.). A descriptive statistic that summarizes the relationship between two measures, all in one number, was invented by Karl Pearson. Everyone now calls it, “Pearson’s <span class="math inline">\(r\)</span>”.</p>
<blockquote>
<p>Karl Pearson was a big-wig editor at Biometrika in the 1930s and all around horrible person. He took a hating to another big-wig statistician, Sir Ronald Fisher (who we learn about later, but was also generally a horrible person), and they had some stats fights…why can’t we all just get along in statistics? Seen as the founding fathers of modern statistics, both Pearson and Fisher held deeply racist views and actively contributed to the field of eugenics. You can read more about the troubled history of modern statistics in <a href="https://statmodeling.stat.columbia.edu/2020/08/01/ra-fisher-and-the-science-of-hatred/">many</a>, <a href="https://www.bayesianspectacles.org/karl-pearsons-worst-quotation/">many</a> different <a href="https://njoselson.github.io/Fisher-Pearson/">places</a>. For now, we will focus about how different statistics work and how to use statistics (for good, hopefully), but I think it’s important to keep this troubled history in mind.</p>
</blockquote>
<p>How does Pearson’s <span class="math inline">\(r\)</span> work? Let’s look again at the first 10 subjects in our fake experiment:</p>
<table>
<thead>
<tr class="header">
<th align="left">subject</th>
<th align="left">chocolate</th>
<th align="left">happiness</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">2</td>
<td align="left">2</td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td align="left">2</td>
<td align="left">2</td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="left">3</td>
<td align="left">2</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="left">3</td>
<td align="left">5</td>
</tr>
<tr class="even">
<td align="left">6</td>
<td align="left">4</td>
<td align="left">5</td>
</tr>
<tr class="odd">
<td align="left">7</td>
<td align="left">5</td>
<td align="left">5</td>
</tr>
<tr class="even">
<td align="left">8</td>
<td align="left">7</td>
<td align="left">6</td>
</tr>
<tr class="odd">
<td align="left">9</td>
<td align="left">5</td>
<td align="left">5</td>
</tr>
<tr class="even">
<td align="left">10</td>
<td align="left">9</td>
<td align="left">8</td>
</tr>
<tr class="odd">
<td align="left">Sums</td>
<td align="left">41</td>
<td align="left">41</td>
</tr>
<tr class="even">
<td align="left">Means</td>
<td align="left">4.1</td>
<td align="left">4.1</td>
</tr>
</tbody>
</table>
<p>What could we do to these numbers to produce a single summary value that represents the relationship between the chocolate supply and happiness?</p>
<div id="the-idea-of-co-variance" class="section level3">
<h3><span class="header-section-number">3.2.1</span> The idea of co-variance</h3>
<p>“Oh please no, don’t use the word variance again”. Yes, we’re doing it, we’re going to use the word variance again, and again, until it starts making sense. Remember what variance means about some numbers. It means the numbers have some change in them, they are not all the same, some of them are big, some are small. We can see that there is variance in chocolate supply across the 10 subjects. We can see that there is variance in happiness across the 10 subjects. We also saw in the scatter plot, that happiness increases as chocolate supply increases; which is a positive relationship, a positive correlation. What does this have to do with variance? Well, it means there is a relationship between the variance in chocolate supply, and the variance in happiness levels. The two measures vary together don’t they? When we have two measures that vary together, they are like a happy couple who share their variance. This is what co-variance refers to, the idea that the pattern of varying numbers in one measure is shared by the pattern of varying numbers in another measure.</p>
<p><strong>Co-variance</strong> is <strong>very, very, very, very</strong> important. We suspect that the word co-variance is initially confusing, especially if you are not yet fully comfortable with the meaning of variance for a single measure. Nevertheless, we must proceed and use the idea of co-variance over and over again to firmly implant it into your statistical mind (we already said, but redundancy works, it’s a thing).</p>
<blockquote>
<p>Pro tip: A <a href="https://en.wikipedia.org/wiki/Three-legged_race">three-legged race</a> is a metaphor for co-variance. Two people tie one leg to each other, then try to walk. It works when they co-vary their legs together (positive relationship). They can also co-vary in an unhelpful way, when one person tries to move forward exactly when the other person tries to move backward. This is still co-variance (negative relationship). Funny random walking happens when there is no co-variance. This means one person does whatever they want, and so does the other person. There is a lot of variance, but the variance is shared randomly, so it’s just a bunch of legs moving around accomplishing nothing.</p>
</blockquote>
<blockquote>
<p>Pro tip #2: Succesfully playing <a href="https://en.wikipedia.org/wiki/Pat-a-cake,_pat-a-cake,_baker%27s_man#Game">pattycake</a> occurs when two people coordinate their actions so they have postively shared co-variance.</p>
</blockquote>
</div>
</div>
<div id="turning-the-numbers-into-a-measure-of-co-variance" class="section level2">
<h2><span class="header-section-number">3.3</span> Turning the numbers into a measure of co-variance</h2>
<p>“OK, so you are saying that co-variance is just another word for the association or relationship between two measures. I suppose we could use some way to measure that.” Correct, back to our table…notice anything new?</p>
<table>
<thead>
<tr class="header">
<th align="left">subject</th>
<th align="left">chocolate</th>
<th align="left">happiness</th>
<th align="left">Chocolate_X_Happiness</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">2</td>
<td align="left">2</td>
<td align="left">4</td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td align="left">2</td>
<td align="left">2</td>
<td align="left">4</td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="left">3</td>
<td align="left">2</td>
<td align="left">6</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="left">3</td>
<td align="left">5</td>
<td align="left">15</td>
</tr>
<tr class="even">
<td align="left">6</td>
<td align="left">4</td>
<td align="left">5</td>
<td align="left">20</td>
</tr>
<tr class="odd">
<td align="left">7</td>
<td align="left">5</td>
<td align="left">5</td>
<td align="left">25</td>
</tr>
<tr class="even">
<td align="left">8</td>
<td align="left">7</td>
<td align="left">6</td>
<td align="left">42</td>
</tr>
<tr class="odd">
<td align="left">9</td>
<td align="left">5</td>
<td align="left">5</td>
<td align="left">25</td>
</tr>
<tr class="even">
<td align="left">10</td>
<td align="left">9</td>
<td align="left">8</td>
<td align="left">72</td>
</tr>
<tr class="odd">
<td align="left">Sums</td>
<td align="left">41</td>
<td align="left">41</td>
<td align="left">214</td>
</tr>
<tr class="even">
<td align="left">Means</td>
<td align="left">4.1</td>
<td align="left">4.1</td>
<td align="left">21.4</td>
</tr>
</tbody>
</table>
<p>We’ve added a new column called “Chocolate_X_Happiness”, which translates to Chocolate scores multiplied by Happiness scores. Each row in the new column, is the product, or multiplication of the chocolate and happiness score for that row. Why would we do this?</p>
<p>Last chapter we took you back to Elementary school and had you think about division. Now it’s time to do the same thing with multiplication. One number times another, means taking the first number, and adding it as many times as the second says to do,</p>
<p><span class="math inline">\(2*2= 2+2=4\)</span></p>
<p><span class="math inline">\(2*6= 2+2+2+2+2+2 = 12\)</span>, or <span class="math inline">\(6+6=12\)</span>, same thing.</p>
<p>You know all that. All we have to think about next are the consequences of multiplying sets of numbers together. For example, what happens when you multiply two small numbers together, compared to multiplying two big numbers together? The first product should be smaller than the second product right? How about things like multiplying a small number by a big number? Those products should be in between right?.</p>
<p>The next step is to think about how the products of two measures sum together, depending on how they line up. Let’s look at another table:</p>
<table>
<thead>
<tr class="header">
<th align="left">scores</th>
<th align="left">X</th>
<th align="left">Y</th>
<th align="left">A</th>
<th align="left">B</th>
<th align="left">XY</th>
<th align="left">AB</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">10</td>
<td align="left">1</td>
<td align="left">10</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">2</td>
<td align="left">2</td>
<td align="left">2</td>
<td align="left">9</td>
<td align="left">4</td>
<td align="left">18</td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td align="left">3</td>
<td align="left">3</td>
<td align="left">3</td>
<td align="left">8</td>
<td align="left">9</td>
<td align="left">24</td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="left">4</td>
<td align="left">4</td>
<td align="left">4</td>
<td align="left">7</td>
<td align="left">16</td>
<td align="left">28</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="left">5</td>
<td align="left">5</td>
<td align="left">5</td>
<td align="left">6</td>
<td align="left">25</td>
<td align="left">30</td>
</tr>
<tr class="even">
<td align="left">6</td>
<td align="left">6</td>
<td align="left">6</td>
<td align="left">6</td>
<td align="left">5</td>
<td align="left">36</td>
<td align="left">30</td>
</tr>
<tr class="odd">
<td align="left">7</td>
<td align="left">7</td>
<td align="left">7</td>
<td align="left">7</td>
<td align="left">4</td>
<td align="left">49</td>
<td align="left">28</td>
</tr>
<tr class="even">
<td align="left">8</td>
<td align="left">8</td>
<td align="left">8</td>
<td align="left">8</td>
<td align="left">3</td>
<td align="left">64</td>
<td align="left">24</td>
</tr>
<tr class="odd">
<td align="left">9</td>
<td align="left">9</td>
<td align="left">9</td>
<td align="left">9</td>
<td align="left">2</td>
<td align="left">81</td>
<td align="left">18</td>
</tr>
<tr class="even">
<td align="left">10</td>
<td align="left">10</td>
<td align="left">10</td>
<td align="left">10</td>
<td align="left">1</td>
<td align="left">100</td>
<td align="left">10</td>
</tr>
<tr class="odd">
<td align="left">Sums</td>
<td align="left">55</td>
<td align="left">55</td>
<td align="left">55</td>
<td align="left">55</td>
<td align="left">385</td>
<td align="left">220</td>
</tr>
<tr class="even">
<td align="left">Means</td>
<td align="left">5.5</td>
<td align="left">5.5</td>
<td align="left">5.5</td>
<td align="left">5.5</td>
<td align="left">38.5</td>
<td align="left">22</td>
</tr>
</tbody>
</table>
<p>Look at the X and Y column. The scores for X and Y perfectly co-vary. When X is 1, Y is 1; when X is 2, Y is 2, etc. They are perfectly aligned. The scores for A and B also perfectly co-vary, just in the opposite manner. When A is 1, B is 10; when A is 2, B is 9, etc. B is a reversed copy of A.</p>
<p>Now, look at the column <span class="math inline">\(XY\)</span>. These are the products we get when we multiply the values of X across with the values of Y. Also, look at the column <span class="math inline">\(AB\)</span>. These are the products we get when we multiply the values of A across with the values of B. So far so good.</p>
<p>Now, look at the <code>Sums</code> for the XY and AB columns. Not the same. The sum of the XY products is 385, and the sum of the AB products is 220. For this specific set of data, the numbers 385 and 220 are very important. They represent the biggest possible sum of products (385), and the smallest possible sum of products (220). There is no way of re-ordering the numbers 1 to 10, say for X, and the numbers 1 to 10 for Y, that would ever produce larger or smaller numbers. Don’t believe me? Check this out:</p>
<p><img src="quantrma_files/figure-html/3simsum-1.png" width="672" /></p>
<p>The above graph shows 1000 computer simulations. I convinced my computer to randomly order the numbers 1 to 10 for X, and randomly order the numbers 1 to 10 for Y. Then, I multiplied X and Y, and added the products together. I did this 1000 times. The dots show the sum of the products for each simulation. The two red lines show the maximum possible sum (385), and the minimum possible sum (220), for this set of numbers. Notice, how all of the dots are in between the maximum and minimum possible values.</p>
<p>“Who cares?”. We’ve been looking for a way to summarize the co-variance between two measures. Well, for these numbers, we have found one. It’s the sum of the products. We know that when the sum of the products is 385, we have found a perfect, positive relationship We know that when the sum of the products is 220, we have found a perfect negative relationship. What about the numbers in between? What could we conclude about the relationship if we found the sum of the products to be 350. Well, it’s going to be positive, because it’s close to 385, and that’s perfectly positive. If the sum of the products was 240, that’s going to be negative, because it’s close to the perfectly negative 220. What about no relationship? Well, that’s going to be in the middle between 220 and 385.</p>
<p>We have just come up with a data-specific summary measure for the relationship between the numbers 1 to 10 in X, and the numbers 1 to 10 in Y: it’s the sum of the products. We know the maximum (385) and minimum values (220), so we can now interpret any product sum for this kind of data with respect to that scale.</p>
<blockquote>
<p>Pro tip: When the relationship between two measures increases in the positive direction, the sum of their products increases to its maximum possible value. This is because the bigger numbers in X will tend to line up with the bigger numbers in Y, creating the biggest possible sum of products. When the relationship between two measures increases in the negative direction, the sum of their products decreases to its minimum possible value. This is because the bigger numbers in X will tend to line up with the smaller numbers in Y, creating the smallest possible sum of products. When there is no correlation, the big numbers in X will be randomly lined up with the big and small numbers in Y, making the sum of the products, somewhere in the middle.</p>
</blockquote>
<div id="co-variance-the-measure" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Co-variance, the measure</h3>
<p>We took some time to see what happens when you multiply sets of numbers together. We found that <span class="math inline">\(big*big = bigger\)</span> and <span class="math inline">\(small*small=\text{still small}\)</span>, and <span class="math inline">\(big*small=\text{in the middle}\)</span>. The purpose of this was to give you some conceptual idea of how the co-variance between two measures is reflected in the sum of their products. We did something very straightforward. We just multiplied X with Y, and looked at how the product sums get big and small, as X and Y co-vary in different ways.</p>
<p>Now, we can get a little bit more formal. In statistics, <strong>co-variance</strong> is not just the straight multiplication of values in X and Y. Instead, it’s the multiplication of the deviations in X from the mean of X, and the deviation in Y from the mean of Y. Remember those difference scores from the mean we talked about last chapter? They’re coming back to haunt you know, but in a good way like Casper the friendly ghost.</p>
<p>Let’s see what this look like in a table:</p>
<table>
<thead>
<tr class="header">
<th align="left">subject</th>
<th align="left">chocolate</th>
<th align="left">happiness</th>
<th align="left">C_d</th>
<th align="left">H_d</th>
<th align="left">Cd_x_Hd</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">-3.1</td>
<td align="left">-3.1</td>
<td align="left">9.61</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">2</td>
<td align="left">2</td>
<td align="left">-2.1</td>
<td align="left">-2.1</td>
<td align="left">4.41</td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td align="left">2</td>
<td align="left">2</td>
<td align="left">-2.1</td>
<td align="left">-2.1</td>
<td align="left">4.41</td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="left">3</td>
<td align="left">2</td>
<td align="left">-1.1</td>
<td align="left">-2.1</td>
<td align="left">2.31</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="left">3</td>
<td align="left">5</td>
<td align="left">-1.1</td>
<td align="left">0.9</td>
<td align="left">-0.99</td>
</tr>
<tr class="even">
<td align="left">6</td>
<td align="left">4</td>
<td align="left">5</td>
<td align="left">-0.1</td>
<td align="left">0.9</td>
<td align="left">-0.09</td>
</tr>
<tr class="odd">
<td align="left">7</td>
<td align="left">5</td>
<td align="left">5</td>
<td align="left">0.9</td>
<td align="left">0.9</td>
<td align="left">0.81</td>
</tr>
<tr class="even">
<td align="left">8</td>
<td align="left">7</td>
<td align="left">6</td>
<td align="left">2.9</td>
<td align="left">1.9</td>
<td align="left">5.51</td>
</tr>
<tr class="odd">
<td align="left">9</td>
<td align="left">5</td>
<td align="left">5</td>
<td align="left">0.9</td>
<td align="left">0.9</td>
<td align="left">0.81</td>
</tr>
<tr class="even">
<td align="left">10</td>
<td align="left">9</td>
<td align="left">8</td>
<td align="left">4.9</td>
<td align="left">3.9</td>
<td align="left">19.11</td>
</tr>
<tr class="odd">
<td align="left">Sums</td>
<td align="left">41</td>
<td align="left">41</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">46</td>
</tr>
<tr class="even">
<td align="left">Means</td>
<td align="left">4.1</td>
<td align="left">4.1</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">4.59</td>
</tr>
</tbody>
</table>
<p>We have computed the deviations from the mean for the chocolate scores (column <code>C_d</code>), and the deviations from the mean for the happiness scores (column <code>H_d</code>). Then, we multiplied them together (last column). Finally, you can see the mean of the products listed in the bottom right corner of the table, <strong>the covariance</strong>.</p>
<p>The formula for the co-variance is:</p>
<p><span class="math inline">\(cov(X,Y) = \frac{\sum_{i}^{n}(x_{i}-\bar{X})(y_{i}-\bar{Y})}{N}\)</span></p>
<p>These are simply the steps we just took, but in the form of a formula. OK, so now we have a formal single number to calculate the relationship between two variables. This is great, it’s what we’ve been looking for. However, there is still a problem. Remember when we learned how to compute just the plain old <strong>variance</strong>. We looked at that number, and we didn’t know what to make of it. It was squared, it wasn’t in the same scale as the original data. So, we square rooted the <strong>variance</strong> to produce the <strong>standard deviation</strong>, which gave us a more interpretable number in the range of our data. The <strong>co-variance</strong> has a similar problem. When you calculate the co-variance as we just did, we don’t know immediately know its scale. Is a 3 big? is a 6 big? is a 100 big? How big or small is this thing?</p>
<p>From our prelude discussion on the idea of co-variance, we learned the sum of products between two measures ranges between a maximum and minimum value. The same is true of the co-variance. For a given set of data, there is a maximum possible positive value for the co-variance (which occurs when there is perfect positive relationship). And, there is a minimum possible negative value for the co-variance (which occurs when there is a perfect negative relationship). When there is zero co-variation, guess what happens: zeroes. So, at the very least, when we look at a co-variation statistic, we can see what direction it points, positive or negative. But, we don’t know how big or small it is compared to the maximum or minimum possible value, so we don’t know the relative size, which means we can’t say how strong the relationship is. What to do?</p>
</div>
<div id="r-we-there-yet" class="section level3">
<h3><span class="header-section-number">3.3.2</span> r we there yet?</h3>
<p>Yes, we are here now. Wouldn’t it be nice if we could force our measure of co-variation to be between -1 and +1?</p>
<p>-1 would be the minimum possible value for a perfect negative association. +1 would be the maximum possible value for a perfect positive association. 0 would mean no association. Everything in between 0 and -1 would be increasingly large negative associations. Everything between 0 and +1 would be increasingly large positive associations. It would be a fantastic, sensible, easy to interpret system. If only we could force the co-variation number to be between -1 and 1. Fortunately, Pearson’s <span class="math inline">\(r\)</span> does precisely this wonderful thing.</p>
<p>Let’s take a look at a formula:</p>
<p><span class="math inline">\(r = \frac{cov(X,Y)}{\sigma_{X}\sigma_{Y}} = \frac{cov(X,Y)}{SD_{X}SD_{Y}}\)</span></p>
<p>We see the symbol <span class="math inline">\(\sigma\)</span> here, that’s more Greek for you. <span class="math inline">\(\sigma\)</span> is often used as a symbol for the standard deviation (SD). If we read out the formula in English, we see that r is the co-variance of X and Y, divided by the product of the standard deviation of X and the standard deviation of Y. Why are we dividing the co-variance by the product of the standard deviations. This operation has the effect of <strong>normalizing</strong> the co-variance into the range -1 to 1.</p>
<div class="marginnote">
<p>But, we will fill this part in as soon as we can…promissory note to explain the magic. FYI, it’s not magic. Brief explanation here is that dividing each measure by its standard deviation ensures that the values in each measure are in the same range as one another.</p>
</div>
<p>For now, we will call this mathematical magic. It works, but we don’t have space to tell you why it works right now.</p>
<blockquote>
<p>It’s worth saying that there are loads of different formulas for computing Pearson’s <span class="math inline">\(r\)</span>. You can find them by Googling them. We will probably include more of them here, when we get around to it. However, they all give you the same answer. And, they are all not as pretty as each other. Some of them might even look scary. In other statistics textbook you will often find formulas that are easier to use for calculation purposes. For example, if you only had a pen and paper, you might use one or another formula because it helps you compute the answer faster by hand. To be honest, we are not very interested in teaching you how to plug numbers into formulas. We give one lesson on that here: Put the numbers into the letters, then compute the answer. Sorry to be snarky. Nowadays you have a computer that you should use for this kind of stuff. So, we are more interested in teaching you what the calculations mean, rather than how to do them. Of course, every week we are showing you how to do the calculations in class with computers.</p>
</blockquote>
<p>Does Pearson’s <span class="math inline">\(r\)</span> really stay between -1 and 1 no matter what? It’s true, take a look at the following simulation. Here I randomly ordered the numbers 1 to 10 for an X measure, and did the same for a Y measure. Then, I computed Pearson’s <span class="math inline">\(r\)</span>, and repeated this process 1000 times. As you can see all of the dots are between -1 and 1. Neat huh.</p>
<div class="figure"><span id="fig:3onethousandr"></span>
<img src="quantrma_files/figure-html/3onethousandr-1.png" alt="A simulation of of correlations. Each dot represents the r-value for the correlation between an X and Y variable that each contain the numbers 1 to 10 in random orders. The figure ilustrates that many r-values can be obtained by this random process" width="672" />
<p class="caption">
Figure 3.3: A simulation of of correlations. Each dot represents the r-value for the correlation between an X and Y variable that each contain the numbers 1 to 10 in random orders. The figure ilustrates that many r-values can be obtained by this random process
</p>
</div>
</div>
</div>
<div id="examples-with-data" class="section level2">
<h2><span class="header-section-number">3.4</span> Examples with Data</h2>
<p>During the in-class exercises you will be shown how to compute correlations in real data-sets using software. To give you a brief preview, let’s look at some data from the <a href="http://worldhappiness.report">world happiness report</a> (2018).</p>
<p>This report measured various attitudes across people from different countries over the time period 2006-2017. For example, one question asked about how much freedom people thought they had to make life choices. Another question asked how confident people were in their national government. Here is a scatterplot showing the relationship between these two measures. Each dot represents means for different countries over the years 2006-2017.</p>
<div class="figure"><span id="fig:3hsrdata"></span>
<img src="quantrma_files/figure-html/3hsrdata-1.png" alt="Relationship between freedom to make life choices and confidence in national government. Data from the world happiness report for 2018" width="672" />
<p class="caption">
Figure 3.4: Relationship between freedom to make life choices and confidence in national government. Data from the world happiness report for 2018
</p>
</div>
<p>We put a blue line on the scatterplot to visualize the positive relationship. It appears that as “freedom to make life choices goes up”, so to does confidence in national government. It’s a positive correlation.</p>
<p>The actual correlation, as measured by Pearson’s <span class="math inline">\(r\)</span> is:</p>
<pre><code>## [1] 0.4080963</code></pre>
<p>You will do a lot more of this kind of thing during the exercises. Looking at the graph you might start to wonder: Does freedom to make life choices cause changes how confident people are in their national government? Our does it work the other way? Does being confident in your national government give you a greater sense of freedom to make life choices? Or, is this just a random relationship that doesn’t mean anything? All good questions. These data do not provide the answers, they just suggest a possible relationship.</p>
</div>
<div id="interpreting-correlations" class="section level2">
<h2><span class="header-section-number">3.5</span> Interpreting Correlations</h2>
<p>What does the presence or the absence of a correlation between two measures mean? How should correlations be interpreted? What kind of inferences can be drawn from correlations? These are all very good questions. A first piece of advice is to use caution when interpreting correlations. Here’s why.</p>
<div id="correlation-does-not-equal-causation" class="section level3">
<h3><span class="header-section-number">3.5.1</span> Correlation does not equal causation</h3>
<p>Perhaps you have heard that correlation does not equal causation. There are lots of reasons why not. However, before listing some of the reasons let’s start with a case where we would expect a causal connection between two measurements. Consider, buying a <a href="https://en.wikipedia.org/wiki/Dracaena_trifasciata">snake plant</a> for your home. Snake plants are supposed to be easy to take care of because you can mostly ignore them.</p>
<p>Like most plants, snake plants need some water to stay alive. However, they also need just the right amount of water. Imagine an experiment where 1000 snake plants were grown in a house. Each snake plant is given a different amount of water per day, from zero teaspoons of water per day to 1000 teaspoons of water per day. We will assume that water is part of the causal process that allows snake plants to grow. The amount of water given to each snake plant per day can also be one of our measures. Imagine further that every week the experimenter measures snake plant growth, which will be the second measurement. Now, can you imagine for yourself what a scatter plot of weekly snake plant growth by tablespoons of water would look like?</p>
<div id="even-when-there-is-causation-there-might-not-be-obvious-correlation" class="section level4">
<h4><span class="header-section-number">3.5.1.1</span> Even when there is causation, there might not be obvious correlation</h4>
<p>The first plant given no water at all would have a very hard time and eventually die. It should have the least amount of weekly growth. How about the plants given only a few teaspoons of water per day. This could be just enough water to keep the plants alive, so they will grow a little bit but not a lot. If you are imagining a scatter plot, with each dot being a snake plant, then you should imagine some dots starting in the bottom left hand corner (no water &amp; no plant growth), moving up and to the right (a bit of water, and a bit of growth). As we look at snake plants getting more and more water, we should see more and more plant growth, right? But only up to a point… There’s as likely positive correlation with increasing plant growth as amount of water per day increases. But, what happens when you give snake plants too much water? From personal experience, they tend to die. So, at some point, the dots in the scatter plot will start moving back down again. Snake plants that get way too much water will not grow very well.</p>
<p>The imaginary scatter plot you should be envisioning could have an upside U shape. Going from left to right, the dot’s go up, they reach a maximum, then they go down again reaching a minimum. Computing Pearson’s <span class="math inline">\(r\)</span> for data like this gives you <span class="math inline">\(r\)</span> values close to zero, while the scatter plot could look something like this:</p>
<div class="figure"><span id="fig:3snakeplant"></span>
<img src="quantrma_files/figure-html/3snakeplant-1.png" alt="Illustration of a possible relationship between amount of water and snake plant growth. Growth goes up with water, but eventually goes back down as too much water makes snake plants die." width="672" />
<p class="caption">
Figure 3.5: Illustration of a possible relationship between amount of water and snake plant growth. Growth goes up with water, but eventually goes back down as too much water makes snake plants die.
</p>
</div>
<p>Granted this looks more like an inverted V, than an inverted U, but you get the picture right? There is clearly a relationship between watering and snake plant growth. But, the relationship isn’t in one direction. As a result, when we compute the correlation in terms of Pearson’s r, we get a value suggesting no relationship.</p>
<pre><code>## [1] 0.001995951</code></pre>
<p>What this really means is there is no linear relationship that can be described by a single straight line. When we need lines or curves going in more than one direction, we have a nonlinear relationship.</p>
<p>This example illustrates some conundrums in interpreting correlations. We already know that water is needed for plants to grow, so we are rightly expecting there to be a relationship between our measure of amount of water and plant growth. If we look at the first half of the data we see a positive correlation, if we look at the last half of the data we see a negative correlation, and if we look at all of the data we see no correlation. Yikes. So, even when there is a causal connection between two measures, we won’t necessarily obtain clear evidence of the connection just by computing a correlation coefficient.</p>
<blockquote>
<p>Pro tip: This is one reason why plotting your data is so important. If you see an upside U shape pattern, then a correlation is probably not the best way of analyzing your data.</p>
</blockquote>
</div>
<div id="confounding-variable-or-the-third-variable-problem" class="section level4">
<h4><span class="header-section-number">3.5.1.2</span> Confounding variable or the third variable problem</h4>
<p>Anybody can correlate any two things that can be quantified and measured. For example, we could find a hundred people, ask them all sorts of questions like:</p>
<ol style="list-style-type: decimal">
<li>how happy are you</li>
<li>how old are you</li>
<li>how tall are you</li>
<li>how much money do you make per year</li>
<li>how long are your eyelashes</li>
<li>how many books have you read in your life</li>
<li>how loud is your inner voice</li>
</ol>
<p>Let’s say we found a positive correlation between yearly salary and happiness. Note, we could have just as easily computed the same correlation between happiness and yearly salary. If we found a correlation, would you be willing to infer that yearly salary causes happiness? Perhaps it does play a part. But, something like happiness probably has a lot of contributing causes. Money could directly cause some people to be happy. But, more likely, money buys people access to all sorts of things, and some of those things might contribute happiness. These “other” things are called <strong>third</strong> variables. For example, perhaps people living in nicer places in more expensive houses are more happy than people in worse places in cheaper houses. In this scenario, money isn’t causing happiness, it’s the places and houses that money buys. But, even is this were true, people can still be more or less happy in lots of different situations.</p>
<p>A more absurd example comes from the (seemingly) very strong correlation between ice cream sales and shark attacks:</p>
<div class="figure"><span id="fig:3ice-shark"></span>
<img src="figures/ice-shark.png" alt="The relationship between ice cream sales and shark attacks?"  />
<p class="caption">
Figure 3.6: The relationship between ice cream sales and shark attacks?
</p>
</div>
<p>If you were to calculate the correlation between ice cream sales and shark attacks you would find a value close to 1, indicating a near perfect positive relationship between the two variables. Are sharks allergic to ice cream and taking out their frustration on random swimmers-by? Should we ban ice cream in beach towns for public safety reasons? What is going on? Here, as well, the relationship is better explained by a third variable that wasn’t measured: during hot weather people are more likely to buy ice cream and also more likely to go swimming, increasing the overall likelihood of being attacked by a shark.</p>
<p>The lesson here is that a correlation can occur between two measures because of a third variable that is not directly measured. So, just because we find a correlation, does not mean we can conclude anything about a causal connection between two measurements.</p>
</div>
</div>
<div id="correlation-and-random-chance" class="section level3">
<h3><span class="header-section-number">3.5.2</span> Correlation and random chance</h3>
<p>Another very important aspect of correlations is the fact that they can be produced by random chance. This means that you can find a positive or negative correlation between two measures, even when they have absolutely nothing to do with one another. You might have hoped to find zero correlation when two measures are totally unrelated to each other. Although this certainly happens, unrelated measures can accidentally produce <strong>spurious</strong> correlations, just by chance alone.</p>
<p>Let’s demonstrate how correlations can occur by chance when there is no causal connection between two measures. Imagine two participants. One is at the North pole with a lottery machine full of balls with numbers from 1 to 10. The other is at the south pole with a different lottery machine full of balls with numbers from 1 to 10. There are an endless supply of balls in the machine, so every number could be picked for any ball. Each participant randomly chooses 10 balls, then records the number on the ball. In this situation we will assume that there is no possible way that balls chosen by the first participant could causally influence the balls chosen by the second participant. They are on the other side of the world. We should assume that the balls will be chosen by chance alone.</p>
<p>Here is what the numbers on each ball could look like for each participant:</p>
<table>
<thead>
<tr class="header">
<th align="right">Ball</th>
<th align="right">North_pole</th>
<th align="right">South_pole</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">10</td>
<td align="right">6</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">8</td>
<td align="right">9</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">4</td>
<td align="right">9</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">8</td>
<td align="right">3</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">7</td>
<td align="right">8</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">9</td>
<td align="right">5</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="right">6</td>
<td align="right">9</td>
</tr>
<tr class="even">
<td align="right">8</td>
<td align="right">4</td>
<td align="right">5</td>
</tr>
<tr class="odd">
<td align="right">9</td>
<td align="right">5</td>
<td align="right">8</td>
</tr>
<tr class="even">
<td align="right">10</td>
<td align="right">9</td>
<td align="right">6</td>
</tr>
</tbody>
</table>
<p>In this one case, if we computed Pearson’s <span class="math inline">\(r\)</span>, we would find that <span class="math inline">\(r =\)</span> -0.3678062. But, we already know that this value does not tell us anything about the relationship between the balls chosen in the north and south pole. We know that relationship is completely random, because that is how we set up the game.</p>
<p>The better question here is to ask what can random chance do? For example, if we ran our game over and over again thousands of times, each time choosing new balls, and each time computing the correlation, what would we find? The <span class="math inline">\(r\)</span> value will sometimes be positive, sometimes be negative, sometimes be big and sometimes be small. Let’s look at what this random fluctuations would look like. This will give us a window into the kinds of correlations that chance alone can produce.</p>
<div id="monte-carlo-simulation-of-random-correlations" class="section level4">
<h4><span class="header-section-number">3.5.2.1</span> Monte-carlo simulation of random correlations</h4>
<p>It is possible to use a computer to simulate our game as many times as we want. This process is termed a <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method"><strong>monte-carlo simulation</strong></a>, after the code name for this procedure during the <a href="https://www.degruyter.com/view/journals/mcma/22/1/article-p73.xml">Manhattan project</a>.</p>
<p>Below is a script written for the programming language R. We won’t go into the details of the code here. However, let’s briefly explain what is going on. Notice, the part that says <code>for(sim in 1:1000)</code>. This creates a loop that repeats our game 1000 times. Inside the loop there are variables named <code>North_pole</code> and <code>South_pole</code>. During each simulation, we sample 10 random numbers (between 1 to 10) using <code>runif(10,1,10)</code> into each variable. These random numbers stand for the numbers that would have been on the balls from the lottery machine. Once we have 10 random numbers for each, we compute the correlation using <code>cor(North_pole,South_pole)</code>. Then, we save the correlation value and move on to the next simulation. At the end, we will have 1000 individual Pearson <span class="math inline">\(r\)</span> values.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="CorrelationCausation.html#cb3-1"></a>simulated_correlations &lt;-<span class="st"> </span><span class="kw">length</span>(<span class="dv">0</span>)</span>
<span id="cb3-2"><a href="CorrelationCausation.html#cb3-2"></a><span class="cf">for</span>(sim <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">1000</span>){</span>
<span id="cb3-3"><a href="CorrelationCausation.html#cb3-3"></a>  North_pole &lt;-<span class="st"> </span><span class="kw">round</span>(<span class="kw">runif</span>(<span class="dv">10</span>,<span class="dv">1</span>,<span class="dv">10</span>))</span>
<span id="cb3-4"><a href="CorrelationCausation.html#cb3-4"></a>  South_pole &lt;-<span class="st"> </span><span class="kw">round</span>(<span class="kw">runif</span>(<span class="dv">10</span>,<span class="dv">1</span>,<span class="dv">10</span>))</span>
<span id="cb3-5"><a href="CorrelationCausation.html#cb3-5"></a>  simulated_correlations[sim] &lt;-<span class="st"> </span><span class="kw">cor</span>(North_pole,South_pole)</span>
<span id="cb3-6"><a href="CorrelationCausation.html#cb3-6"></a>}</span>
<span id="cb3-7"><a href="CorrelationCausation.html#cb3-7"></a></span>
<span id="cb3-8"><a href="CorrelationCausation.html#cb3-8"></a><span class="co"># This is how we generate the figure below</span></span>
<span id="cb3-9"><a href="CorrelationCausation.html#cb3-9"></a>sim_df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">sims=</span><span class="dv">1</span><span class="op">:</span><span class="dv">1000</span>,simulated_correlations)</span>
<span id="cb3-10"><a href="CorrelationCausation.html#cb3-10"></a><span class="kw">ggplot</span>(sim_df, <span class="kw">aes</span>(<span class="dt">x =</span> sims, <span class="dt">y =</span> simulated_correlations))<span class="op">+</span></span>
<span id="cb3-11"><a href="CorrelationCausation.html#cb3-11"></a><span class="st">  </span><span class="kw">geom_point</span>()<span class="op">+</span></span>
<span id="cb3-12"><a href="CorrelationCausation.html#cb3-12"></a><span class="st">  </span><span class="kw">theme_classic</span>()<span class="op">+</span></span>
<span id="cb3-13"><a href="CorrelationCausation.html#cb3-13"></a><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">-1</span>, <span class="dt">color=</span><span class="st">&quot;red&quot;</span>, <span class="dt">linetype =</span> <span class="dv">2</span>)<span class="op">+</span></span>
<span id="cb3-14"><a href="CorrelationCausation.html#cb3-14"></a><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">1</span>, <span class="dt">color=</span><span class="st">&quot;red&quot;</span>, <span class="dt">linetype =</span> <span class="dv">2</span>)<span class="op">+</span></span>
<span id="cb3-15"><a href="CorrelationCausation.html#cb3-15"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Simulation of 1000 r values&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:3anotherthousand"></span>
<img src="quantrma_files/figure-html/3anotherthousand-1.png" alt="Another figure showing a range of r-values that can be obtained by chance" width="672" />
<p class="caption">
Figure 3.7: Another figure showing a range of r-values that can be obtained by chance
</p>
</div>
<p>Let’s take a look at all of the 1000 Pearson <span class="math inline">\(r\)</span> values. Does the figure below look familiar to you? It should, we have already conducted a similar kind of simulation before. Each dot in the scatter plot shows the Pearson <span class="math inline">\(r\)</span> for each simulation from 1 to 1000. As you can see the dots are all over of the place, in between the range -1 to 1. The important lesson here is that random chance produced all of these correlations. This means we can find “correlations” in the data that are completely meaningless, and do not reflect any causal relationship between one measure and another.</p>
<p>Let’s illustrate the idea of finding “random” correlations one more time, with an animation This time, we will show you a scatter plot of the random values sampled for the balls chosen from the North and South pole. If there is no relationship we should see dots going everywhere. If there happens to be a positive relationship (purely by chance), we should see the dots going from the bottom left to the top right. If there happens to be a negative relationship (purely by chance), we should see the dots going from the top left down to the bottom right.</p>
<p>On more thing to prepare you for the animation. There are three scatter plots below, showing negative, positive, and zero correlations between two variables. You’ve already seen this graph before. We are just reminding you that the blue lines are helpful for seeing the correlation.Negative correlations occur when a line goes down from the top left to bottom right. Positive correlations occur when a line goes up from the bottom left to the top right. Zero correlations occur when the line is flat (doesn’t go up or down).</p>
<div class="figure"><span id="fig:3reminder"></span>
<img src="quantrma_files/figure-html/3reminder-1.png" alt="A reminder of what positive, negative, and zero correlation looks like" width="672" />
<p class="caption">
Figure 3.8: A reminder of what positive, negative, and zero correlation looks like
</p>
</div>
<p>OK, now we are ready for the animation. You are looking at the process of sampling two sets of numbers randomly, one for the X variable, and one for the Y variable. Each time we sample 10 numbers for each, plot them, then draw a line through them. Remember, these numbers are all completely random, so we should expect, on average that there should be no correlation between the numbers. However, this is not what happens. You can the line going all over the place. Sometimes we find a negative correlation (line goes down), sometimes we see a positive correlation (line goes up), and sometimes it looks like zero correlation (line is more flat).</p>
<div class="figure"><span id="fig:3randcor10gif"></span>
<img src="gifs/corUnifn10-1.gif" alt="Completely random data points drawn from a uniform distribution with a small sample-size of 10. The blue line twirls around sometimes showing large correlations that are produced by chance"  />
<p class="caption">
Figure 3.9: Completely random data points drawn from a uniform distribution with a small sample-size of 10. The blue line twirls around sometimes showing large correlations that are produced by chance
</p>
</div>
<p>You might be thinking this is kind of disturbing. If we know that there should be no correlation between two random variables, how come we are finding correlations? This is a big problem right? I mean, if someone showed me a correlation between two things, and then claimed one thing was related to another, how could know I if it was true? After all, it could be chance!</p>
<p>Fortunately, all is not lost. We can look at our simulated data in another way, using a histogram. Remember, just before the animation, we simulated 1000 different correlations using random numbers. By, putting all of those <span class="math inline">\(r\)</span> values into a histogram, we can get a better sense of how chance behaves. We can see what kind of correlations chance is likely or unlikely to produce. Here is a histogram of the simulated <span class="math inline">\(r\)</span> values.</p>
<div class="figure"><span id="fig:3histrandcor"></span>
<img src="quantrma_files/figure-html/3histrandcor-1.png" alt="A histogram showing the frequency distribution of r-values for completely random values between an X and Y variable (sample-size=10). A full range of r-values can be obtained by chance alone. Larger r-values are less common than smaller r-values" width="672" />
<p class="caption">
Figure 3.10: A histogram showing the frequency distribution of r-values for completely random values between an X and Y variable (sample-size=10). A full range of r-values can be obtained by chance alone. Larger r-values are less common than smaller r-values
</p>
</div>
<p>Notice that this histogram is not flat. Most of the simulated <span class="math inline">\(r\)</span> values are close to zero. Notice, also that the bars get smaller as you move away from zero in the positive or negative direction. The general take home here is that chance can produce a wide range of correlations. However, not all correlations happen very often. For example, the bars for -1 and 1 are very small. Chance does not produce nearly perfect correlations very often. The bars around -.5 and .5 are smaller than the bars around zero, as medium correlations do not occur as often as small correlations by chance alone.</p>
<p>You can think of this histogram as the window of chance. It shows what chance often does, and what it often does not do. If you found a correlation under these very same circumstances (e.g., measured the correlation between two sets of 10 random numbers), then you could consult this window. What should you ask the window? How about, could my observed correlation (the one that you found in your data) have come from this window?</p>
<p>Let’s say you found a correlation of <span class="math inline">\(r = .1\)</span>. Could a .1 have come from the histogram? Well, look at the histogram around where the .1 mark on the x-axis is. Is there a big bar there? If so, this means that chance produces this value fairly often. You might be comfortable with the inference: Yes, this .1 could have been produced by chance, because it is well inside the window of chance.
How about <span class="math inline">\(r = .5\)</span>? The bar is much smaller here, you might think, “well, I can see that chance does produce .5 some times, so chance could have produced my .5. Did it? Maybe, maybe not, not sure”. Here, your confidence in a strong inference about the role of chance might start getting a bit shakier.</p>
<p>How about an <span class="math inline">\(r = .95\)</span>?. You might see that the bar for .95 is very very small, perhaps too small to see. What does this tell you? It tells you that chance does not produce .95 very often, hardly if at all, pretty much never. So, if you found a .95 in your data, what would you infer? Perhaps you would be comfortable inferring that chance did not produce your .95, after all, .95 is mostly outside the window of chance.</p>
</div>
<div id="increasing-sample-size-decreases-the-opportunity-for-spurious-correlations" class="section level4">
<h4><span class="header-section-number">3.5.2.2</span> Increasing sample-size decreases the opportunity for spurious correlations</h4>
<div class="marginnote">
<p>See <a href="https://www.tylervigen.com/spurious-correlations">this website</a> by Tyler Vigen for some interesting spurious correlations. You can even use his website to <a href="https://tylervigen.com/discover">discover a spurious correlation</a> of your own!</p>
</div>
<p>Before moving on, let’s do one more thing with correlations. In our pretend lottery game, each participant only sampled 10 balls each. We found that this could lead to a range of correlations between the numbers randomly drawn from either sides of the pole. Indeed, we even found some correlations that were medium to large in size. If you were a researcher who found such correlations, you might be tempted to believe there was a relationship between your measurements. However, we know in our little game, that those correlations would be spurious, just a product of random sampling.</p>
<p>The good news is that, as a researcher, you get to make the rules of the game. You get to determine how chance can play. This is all still a little bit metaphorical, so let’s make it concrete.</p>
<p>Let’s what happens in four different scenarios. First, we will repeat what we already did. Each participant will draw 10 balls, then we compute the correlation, and do this over 1000 times and look at a histogram. Second, we will change the game so each participant draws 50 balls each, and then repeat our simulation. Third, and fourth, we will change the game so each participant draws 100 balls each, and then 1000 balls each, and repeat etc.</p>
<p>The graph below shows four different histograms of the Pearson <span class="math inline">\(r\)</span> values in each of the different scenarios. Each scenario involves a different sample-size, from, 10, 50, 100 to 1000.</p>
<div class="figure"><span id="fig:3corrandN"></span>
<img src="quantrma_files/figure-html/3corrandN-1.png" alt="Four histograms showing the frequency distributions of r-values between completely random X and Y variables as a function of sample-size. The width of the distributions shrink as sample-size increases. Smaller sample-sizes are more likely to produce a wider range of r-values by chance. Larger sample-sizes always produce a narrower range of small r-values" width="672" />
<p class="caption">
Figure 3.11: Four histograms showing the frequency distributions of r-values between completely random X and Y variables as a function of sample-size. The width of the distributions shrink as sample-size increases. Smaller sample-sizes are more likely to produce a wider range of r-values by chance. Larger sample-sizes always produce a narrower range of small r-values
</p>
</div>
<p>By inspecting the four histograms you should notice a clear pattern. The width or range of each histogram shrinks as the sample-size increases. What is going on here? Well, we already know that we can think of these histograms as windows of chance. They tell us which <span class="math inline">\(r\)</span> values occur fairly often, which do not. When our sample-size is 10, lots of different <span class="math inline">\(r\)</span> values happen. That histogram is very flat and spread out. However, as the sample-size increases, we see that the window of chance gets pulled in. For example, by the time we get to 1000 balls each, almost all of the Pearson <span class="math inline">\(r\)</span> values are very close to 0.</p>
<p>One take home here, is that increasing sample-size narrows the window of chance. So, for example, if you ran a study involving 1000 samples of two measures, and you found a correlation of .5, then you can clearly see in the bottom right histogram that .5 does not occur very often by chance alone. In fact, there is no bar, because it didn’t happen even once in the simulation. As a result, when you have a large sample size like n = 1000, you might be more confident that your observed correlation (say of .5) was not a spurious correlation. If chance is not producing your result, then something else is.</p>
<p>Finally, notice how your confidence about whether or not chance is mucking about with your results depends on your sample size. If you only obtained 10 samples per measurement, and found <span class="math inline">\(r = .5\)</span>, you should not be as confident that your correlation reflects a real relationship. Instead, you can see that <span class="math inline">\(r\)</span>’s of .5 happen fairly often by chance alone.</p>
<blockquote>
<p>Pro tip: when you run an experiment, under ideal circumstances, you get to decide how many samples you will collect. This means narrowing the window of chance.</p>
</blockquote>
</div>
</div>
<div id="some-more-animations" class="section level3">
<h3><span class="header-section-number">3.5.3</span> Some more animations</h3>
<p>Let’s ingrain these ideas with some more animations. When our sample-size is small (referred to as “n is small”), sampling error can cause all sorts of “patterns” in the data. This makes it possible, and indeed common, for “correlations” to occur between two sets of numbers. When we increase the sample-size, sampling error is reduced, making it less possible for “correlations” to occur just by chance alone.</p>
<div id="watching-how-correlations-behave-when-there-is-no-relationship-between-variables" class="section level4">
<h4><span class="header-section-number">3.5.3.1</span> Watching how correlations behave when there is no relationship between variables</h4>
<p>Below we randomly sample numbers for two variables, plot them, and show the correlation using a line. There are four panels, each showing the number of observations in the samples, from 10, 50, 100, to 1000 in each sample.</p>
<p>Remember, because we are randomly sampling numbers, there should be no relationship between the X and Y variables. But, as we have been discussing, because of chance, we can sometimes observe a correlation (due to chance). The important thing to watch is how the line behaves across the four panels. The line twirls around in all directions when the sample size is 10. It is also moves around quite a bit when the sample size is 50 or 100. It still moves a bit when the sample size is 1000, but much less. In all cases we expect the line to be flat, but sometimes the line shows us a pseudo relationship.</p>
<div class="figure"><span id="fig:3corRandfour"></span>
<img src="gifs/corUnifFourNs-1.gif" alt="Animation of how correlation behaves for completely random X and Y variables as a function of sample size. The best fit line is not very stable for small sample-sizes, but becomes more reliably flat as sample-size increases"  />
<p class="caption">
Figure 3.12: Animation of how correlation behaves for completely random X and Y variables as a function of sample size. The best fit line is not very stable for small sample-sizes, but becomes more reliably flat as sample-size increases
</p>
</div>
<p>Which line should you trust? Well, hopefully you can see that the line for 1000 samples is the most stable. It tends to be very flat every time, and it does not depend so much on the particular sample. The line with 10 observations per sample goes all over the place. The take home here, is that if someone told you that they found a correlation, you should want to know how many observations they had in their sample. If they only had 10 observations, how could you trust the claim that there was a correlation? You can’t!!! Not now that you know samples that are that small can do all sorts of things by chance alone. If instead, you found out the sample was very large, then you might trust that finding a little bit more. For example, in the above animation you can see that when there are 1000 samples, we never see a strong or even weak correlation; the line is almost always completely flat. This is because chance almost never produces strong correlations when the sample size is very large.</p>
<p>OK, so what do things look like when there actually is a correlation between variables?</p>
</div>
<div id="watching-correlations-behave-when-there-really-is-a-correlation" class="section level4">
<h4><span class="header-section-number">3.5.3.2</span> Watching correlations behave when there really is a correlation</h4>
<p>Sometimes there really are correlations between two variables that are not caused by chance. Below, we get to watch an animation of four scatter plots. Each shows the correlation between two variables. Again, we change the sample-size in steps of 10, 50 100, and 1000. The data have been programmed to contain a real positive correlation. So, we should expect that the line will be going up from the bottom left to the top right. However, there is still variability in the data. So this time, sampling error due to chance will fuzz the correlation. We know it is there, but sometimes chance will cause the correlation to be eliminated.</p>
<p>Notice that in the top left panel (sample-size 10), the line is twirling around much more than the other panels. Every new set of samples produces different correlations. Sometimes, the line even goes flat or downward. However, as we increase sample-size, we can see that the line doesn’t change very much, it is always going up showing a positive correlation.</p>
<div class="figure"><span id="fig:3realcorFour"></span>
<img src="gifs/corRealgif-1.gif" alt="How correlation behaves as a function of sample-size when there is a true correlation between X and Y variables"  />
<p class="caption">
Figure 3.13: How correlation behaves as a function of sample-size when there is a true correlation between X and Y variables
</p>
</div>
<p>The main takeaway here is that even when there is a positive correlation between two things, you might not be able to see it if your sample size is small. For example, you might get unlucky with the one sample that you measured. Your sample could show a negative correlation, even when the actual correlation is positive! Unfortunately, in the real world we usually only have the sample that we collected, so we always have to wonder if we got lucky or unlucky. If you want to remove luck, you need to collect larger samples. Then you will be much more likely to observe the real pattern, rather the a pattern that can be introduced by chance.</p>
</div>
</div>
</div>
<div id="summary" class="section level2">
<h2><span class="header-section-number">3.6</span> Summary</h2>
<p>In this section we have talked about correlation, and started to build some intuitions about <strong>inferential statistics</strong>, which is the major topic of the following chapters. For now, the main ideas are:</p>
<ol style="list-style-type: decimal">
<li>We can measure relationships in data using things like correlation</li>
<li>The correlations we measure can be produced by numerous things, so they are sometimes hard to interpret</li>
<li>Correlations can be produced by chance, so have the potential to be completely meaningless</li>
<li>However, we can create a model of exactly what chance can do. The model tells us whether chance is more or less likely to produce correlations of different sizes</li>
<li>We can use the chance model to help us make decisions about our own data. We can compare the correlation we found in our data to the model, then ask whether or not chance could have or was likely to have produced our results.</li>
</ol>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-salsburg2001lady">
<p>Salsburg, David. 2001. <em>The Lady Tasting Tea: How Statistics Revolutionized Science in the Twentieth Century</em>. Macmillan.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="DescribingData.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="for-thanos-to-look-at.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["quantrma.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
